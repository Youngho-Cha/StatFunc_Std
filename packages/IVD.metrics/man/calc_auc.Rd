% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/calc_auc.R
\name{calc_auc}
\alias{calc_auc}
\title{Calculate diagnostic performance metrics}
\usage{
calc_auc(
  actual,
  predicted,
  score = NULL,
  ci_method_auc = "delong",
  alpha = 0.05,
  boot_n = 2000
)
}
\arguments{
\item{actual}{Vectors of actual values(0=negative, 1=positive)}

\item{predicted}{Vector of predicted binary labels}

\item{score}{Optional probability scores for AUC}

\item{alpha}{type I error}

\item{boot_n}{The number of Bootstrap sampling}

\item{ci_method}{Select method to calculate CI for AUC}
}
\value{
Data frame which contains calculated auc and result of pROC::roc
}
\description{
Calculate diagnostic performance metrics
}
\section{Area Under the Curve (AUC) and ROC Curve}{

The Area Under the Receiver Operating Characteristic Curve (AUC-ROC) is a fundamental tool for evaluating the performance of binary classification models. It provides a single, aggregate measure of performance across all possible classification thresholds.

An AUC value represents the model's overall ability to correctly distinguish between the positive and negative classes. It can be interpreted as \strong{the probability that the model will rank a randomly chosen positive instance higher than a randomly chosen negative instance}. A key advantage of AUC is that it is independent of the chosen classification threshold.

\strong{The ROC Curve}

\figure{figure/roc_curve.png}{options: alt="An example of an ROC Curve showing the trade-off between TPR and FPR."}

An ROC curve is a graphical plot that illustrates the diagnostic ability of a binary classifier as its discrimination threshold is varied. It is created by plotting the \strong{True Positive Rate (Sensitivity)} against the \strong{False Positive Rate (1 - Specificity)}.

To generate the curve, the model's continuous output (e.g., a probability score) is treated as a threshold. This threshold is varied from its maximum to its minimum value. At each threshold, a confusion matrix is calculated, yielding a new pair of TPR and FPR values, which are then plotted as a point on the curve.

The axes are defined as:
\itemize{
\item \strong{True Positive Rate (TPR) / Sensitivity (Y-axis):} The proportion of actual positives that are correctly identified as such.
\deqn{TPR = \frac{TP}{TP + FN}}
\item \strong{False Positive Rate (FPR) (X-axis):} The proportion of actual negatives that are incorrectly identified as positive.
\deqn{FPR = \frac{FP}{FP + TN}}
}

\strong{Calculating and Interpreting AUC}

The AUC is the two-dimensional area underneath the entire ROC curve. Numerically, it is calculated by approximating the area using methods like the trapezoidal rule. The value of AUC ranges from 0 to 1, where:
\itemize{
\item \strong{AUC = 1:} Represents a perfect classifier. The ROC curve would pass through the top-left corner (0,1), indicating 100\% sensitivity and 100\% specificity.
\item \strong{AUC = 0.5:} Represents a model with no discriminative ability, equivalent to random guessing. The ROC curve for such a model is a diagonal line from (0,0) to (1,1).
\item \strong{AUC < 0.5:} Represents a model that performs worse than random chance. This often implies that the model's predictions are inverted.
}
}

