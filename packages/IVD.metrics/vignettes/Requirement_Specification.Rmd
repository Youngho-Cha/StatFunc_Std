---
title: "Requirement_Specification"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Requirement_Specification}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup}
# library(IVD.metrics)
```

# 1. calc_binary_metrics

## 1.1 Overview
This function calculates five key binary performance metrics commonly used in the clinical performance evaluation of medical devices: Sensitivity, Specificity, Positive Predictive Value (PPV), Negative Predictive Value (NPV), and Accuracy.\

The function is designed not to introduce a new calculation method, but rather to provide a standardized wrapper for an existing, validated function. The primary goal is to enhance the efficiency of repetitive analytical tasks and ensure consistency across different projects.

## 1.2 Input
* `actual`: 
  + A numeric vector representing the true class labels (i.e., the ground truth). 
  + For binary classification, this vector must consist of 0 (negative cases) and 1 (positive cases).

* `predicted`: 
  + A numeric vector representing the predicted class labels from the model. 
  + This must also be a binary vector consisting of 0s and 1s.

* `metrics_to_calc`: 
  + A character string specifying the performance metric(s) to be calculated. Options are "sensitivity", "specificity", "npv", "ppv", and "accuracy". 
  + If left unspecified, the function will calculate all five metrics by default.

* `ci_method`: 
  + A character string specifying the method for calculating the confidence interval. 
  + Available options are "cp" (for Clopper-Pearson), "wilson" (for Wilson score), and "wald". The default method is "cp".

* `alpha`: 
  + A numeric value specifying the significance level for the confidence interval. 
  + The default is 0.05.
  
## 1.3 Processing Logic
1. Generates a 2x2 confusion matrix based on the `actual` and `predicted` input vectors.

2. Calculates the performance metrics by passing the generated confusion matrix to the `epiR::epi.tests` function.

3. Computes the confidence intervals for the metrics specified in the `metrics_to_calc` argument, using the method defined by the `ci_method` argument.

4. Formats the calculated metrics and their corresponding confidence intervals into a data frame as the final output.

## 1.4 Output
The function returns a list object containing two elements:

1. A 2x2 confusion matrix.
2. A data frame of performance metrics with the following columns:
  - value: The calculated value of the performance metric.
  - lower: The lower bound of the confidence interval for the metric.
  - upper: The upper bound of the confidence interval for the metric.
  
## 1.5 Error & Exception Handling
* **Missing Required Arguments:** 
  + The function will stop execution and throw an explicit error message if the mandatory arguments, `actual` or `predicted`, are not provided.

* **Non-Binary Input:** 
  + If the `actual` or `predicted` vectors contain any values other than 0 and 1, the function will stop and return an error message indicating that the input data is invalid.

* **Invalid Option Values:** 
  + If an unsupported string is passed to the `metrics_to_calc` or `ci_method` arguments, the function will bypass the metrics calculation and return only the 2x2 confusion matrix.
  
# 2. calc_auc

## 2.1 Overview
This function calculates the Area Under the Curve (AUC), a widely used performance metric in the clinical evaluation of medical devices.\ 

The purpose of this function is not to implement a new calculation algorithm but to provide a standardized wrapper for an existing, validated function from the pROC package. This approach aims to improve the efficiency of repetitive analytical tasks and ensure code consistency.

## 2.2 Input
* `actual`: 
  + A numeric vector representing the true class labels (i.e., the ground truth). 
  + This must be a binary vector consisting of 0 (negative cases) and 1 (positive cases).

* `score`: 
  + A numeric vector of predicted scores or probabilities. 
  + These continuous values are used to rank observations for constructing the ROC curve.

* `ci_method`: 
  + A character string specifying the method for calculating the confidence interval. 
  + Options are "delong" or "bootstrap". The default method is "delong".

* `alpha`: 
  + A numeric value specifying the significance level for the confidence interval. 
  + The default is 0.05.

* `boot_n`: 
  + An integer specifying the number of bootstrap replicates to use when ci_method is "bootstrap". 
  + The default is 2000.

## 2.3 Processing Logic
1. Calculates the Area Under the Curve (AUC) by passing the `actual` and `score` vectors to the `pROC::roc` function to generate a ROC curve object.

2. Configures the confidence interval calculation options within the function based on the method specified by the `ci_method` argument.

3. Formats the calculated AUC and its corresponding confidence interval into a data frame, which is then returned as the final output.

## 2.4 Output
The function returns a list object containing two elements:

1. The original roc object generated by the pROC::roc function.
2. A summary data frame with the following columns:
  + value: The calculated AUC value.
  + lower: The lower bound of the confidence interval for the AUC.
  + upper: The upper bound of the confidence interval for the AUC.
  
## 2.5 Error & Exception Handling
* **Missing Required Arguments:** 
  + The function will stop execution and throw an explicit error message if the mandatory arguments, `actual` or `score`, are not provided.

* **Non-Binary actual Input:** 
  + If the `actual` vector contains any values other than 0 and 1, the function will stop and return an error message indicating that the input data is invalid.

* **Unsupported ci_method Value:** 
  + The function will stop and throw an error if the string provided for the `ci_method` argument is not one of the supported options (i.e., "delong" or "bootstrap").
